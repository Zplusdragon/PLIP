# PLIP
**PLIP** is a novel language-image pre-training framework for generic person representation learning which benefits a range of downstream multimodal person-centric tasks. Unlike most existing methods solely relying on visual information, we explore introducing the language modality into person representation learning, drawing inspiration from the intrinsic fine-grained attribute indicators of person descriptions. To explicitly build fine-grained cross-modal associations and mine latent person-centric semantics, we specifically design three pretext tasks: 1) Semantic-fused image colorization, 2) Visual-fused Attribute Prediction and 3) Vision-language Matching.

Also, we present a large-scale person dataset named **SYNTH-PEDES**, where the Stylish Pedestrian Attributes-union Captioning method **(SPAC)** is proposed to synthesize diverse textual descriptions. 

Experiments show that our model not only significantly improves existing methods on downstream tasks, but also shows great ability in the few-shot and domain generalization settings. More details can be found at xxx.

<div align="center"><img src="assets/abstract.png" width="600"></div>
